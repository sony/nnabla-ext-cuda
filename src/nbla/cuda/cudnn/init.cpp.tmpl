// Copyright 2018,2019,2020,2021 Sony Corporation.
// Copyright 2021,2022 Sony Group Corporation.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// *WARNING*
// THIS FILE IS AUTO-GENERATED BY CODE GENERATOR.
// PLEASE DO NOT EDIT THIS FILE BY HAND!
// If you want to modify this file, edit following files.
// - src/nbla/cuda/cudnn/init.cpp.tmpl
// - code_generator/generate.py


#include <nbla/array/cpu_array.hpp>
#include <nbla/array_registry.hpp>
#include <nbla/cuda/array/cuda_array.hpp>
#include <nbla/cuda/cudnn/cudnn.hpp>
#include <nbla/cuda/cudnn/init.hpp>
#include <nbla/function_registry.hpp>
#include <nbla/init.hpp>
#include <nbla/cuda/init.hpp>
#include <nbla/backend_registry.hpp>
% for name, snake_name, _ in function_list:
% if name in function_types:
#include <nbla/cuda/cudnn/function/${snake_name}.hpp>
% endif
% endfor

#include <mutex>

namespace nbla {

void init_cudnn() {
  static std::mutex m;
  std::lock_guard<std::mutex> lock(m);

  static bool is_initialized = false;
  if (is_initialized)
    return;

  // Init CPU features
  init_cpu();
  init_cuda();

  // Backend registration
  NBLA_REGISTER_BACKEND("cudnn", []() { return SingletonManager::get<Cuda>(); });

  // Function registration
% for name, _, arg_types in function_list:
  % for type_config, ttypes in function_types.get(name, {}).items():
    <%
    ttype_args = ', '.join(ttypes)
    ttype_symbol = ''.join(map(lambda x: x.replace(' ', ''), ttypes))
    function_sym = '{}{}<{}>'.format(name, "CudaCudnn", ttype_args)
    function_typed_sym = '{}{}_{}'.format(name, "CudaCudnn", ttype_symbol)
    %> 
  using ${function_typed_sym} = ${function_sym};
  NBLA_REGISTER_FUNCTION_IMPL(${name}, ${function_typed_sym}, "cudnn:${type_config}"${''.join(map(lambda x: ', ' + x, arg_types))});
  % endfor
% endfor

  is_initialized = true;
}

/**
Set conv algo to blacklist.
*/
void set_conv_fwd_algo_blacklist(int id) {
  SingletonManager::get<CudnnHandleManager>()->set_conv_algo_blacklist(id, ConvOpType::FWD);
};

void set_conv_bwd_data_algo_blacklist(int id) {
  SingletonManager::get<CudnnHandleManager>()->set_conv_algo_blacklist(id, ConvOpType::BWD_DATA);
};

void set_conv_bwd_filter_algo_blacklist(int id) {
  SingletonManager::get<CudnnHandleManager>()->set_conv_algo_blacklist(id, ConvOpType::BWD_FILTER);
};

/**
Unset conv algo from blacklist.
*/
void unset_conv_fwd_algo_blacklist(int id) {
    SingletonManager::get<CudnnHandleManager>()->unset_conv_algo_blacklist(id, ConvOpType::FWD);
};

NBLA_CUDA_API void unset_conv_bwd_data_algo_blacklist(int id) {
    SingletonManager::get<CudnnHandleManager>()->unset_conv_algo_blacklist(id, ConvOpType::BWD_DATA);
};

NBLA_CUDA_API void unset_conv_bwd_filter_algo_blacklist(int id){
    SingletonManager::get<CudnnHandleManager>()->unset_conv_algo_blacklist(id, ConvOpType::BWD_FILTER);
};

}
