
// Copyright (c) 2017 Sony Corporation. All Rights Reserved.
// 
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// 
//     http://www.apache.org/licenses/LICENSE-2.0
// 
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// *WARNING*
// THIS FILE IS AUTO-GENERATED BY CODE GENERATOR.
// PLEASE DO NOT EDIT THIS FILE BY HAND!
// If you want to modify this file, edit following files.
// - src/nbla/cuda/init.cpp.tmpl
// - code_generator/generate.py

#include <nbla/init.hpp>
#include <nbla/cuda/init.hpp>
#include <nbla/cuda/cuda.hpp>
#include <nbla/cuda/common.hpp>
#include <nbla/array_registry.hpp>
#include <nbla/function_registry.hpp>
#include <nbla/array/cpu_array.hpp>
#include <nbla/cuda/array/cuda_array.hpp>
% for name, snake_name, _ in function_list:
% if name in function_types:
#include <nbla/cuda/function/${snake_name}.hpp>
% endif
% endfor
#include <nbla/solver_registry.hpp>
% for name, snake_name, _ in solver_list:
% if name in solver_types:
#include <nbla/cuda/solver/${snake_name}.hpp>
% endif
% endfor


#ifdef FEATURE_DIST_TRAIN
  #include <nbla/cuda/communicator/data_parallel_communicator.hpp>
  #include <nbla/cuda/communicator/multi_process_data_parallel_communicator.hpp>
#endif

#include <nbla/garbage_collector.hpp>

namespace nbla {

void init_cuda() {
  static bool is_initialized = false;
  if (is_initialized)
    return;

  // Init CPU features
  init_cpu();

  // Array registration
  // CudaArray
  NBLA_REGISTER_ARRAY_CREATOR(CudaArray);
  SingletonManager::get<Cuda>()->register_array_class("CudaArray");
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuArray, CudaArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CpuArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuCachedArray, CudaArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CpuCachedArray,
                                   synchronizer_cuda_array_cpu_array);

  // CudaCachedArray
  NBLA_REGISTER_ARRAY_CREATOR(CudaCachedArray);
  SingletonManager::get<Cuda>()->register_array_class("CudaCachedArray");
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuArray, CudaCachedArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CpuArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuCachedArray, CudaCachedArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CpuCachedArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CudaArray,
                                   synchronizer_default);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CudaCachedArray,
                                   synchronizer_default);

  // Function registration
% for name, _, arg_types in function_list:
  % for type_config, ttypes in function_types.get(name, {}).items():
    <%
    ttype_args = ', '.join(ttypes)
    ttype_symbol = ''.join(map(lambda x: x.replace(' ', ''), ttypes))
    function_sym = '{}{}<{}>'.format(name, "Cuda", ttype_args)
    function_typed_sym = '{}{}_{}'.format(name, "Cuda", ttype_symbol)
    %> 
  using ${function_typed_sym} = ${function_sym};
  NBLA_REGISTER_FUNCTION_IMPL(${name}, ${function_typed_sym}, "cuda:${type_config}"${''.join(map(lambda x: ', ' + x, arg_types))});
  % endfor
% endfor


  // Solver registration
% for name, _, arg_types in solver_list:
  % for type_config, ttypes in solver_types.get(name, {}).items():
    <%
    ttype_args = ', '.join(ttypes)
    ttype_symbol = ''.join(map(lambda x: x.replace(' ', ''), ttypes))
    solver_sym = '{}{}<{}>'.format(name, "Cuda", ttype_args)
    solver_typed_sym = '{}{}_{}'.format(name, "Cuda", ttype_symbol)
    %> 
  using ${solver_typed_sym} = ${solver_sym};
  NBLA_REGISTER_SOLVER_IMPL(${name}, ${solver_typed_sym}, "cuda:${type_config}"${''.join(map(lambda x: ', ' + x, arg_types))});
  %endfor  
% endfor

  // Communicator registration
#ifdef FEATURE_DIST_TRAIN
  typedef DataParallelCommunicatorNccl<float> DataParallelCommunicatorNcclf;
  NBLA_REGISTER_COMMUNICATOR_IMPL(DataParallelCommunicator, DataParallelCommunicatorNcclf, "cuda:float");
  typedef MultiProcessDataParallelCommunicatorNccl<float> MultiProcessDataParallelCommunicatorNcclf;
  NBLA_REGISTER_COMMUNICATOR_IMPL(MultiProcessDataParallelCommunicator, MultiProcessDataParallelCommunicatorNcclf, "cuda:float");

  typedef DataParallelCommunicatorNccl<Half> DataParallelCommunicatorNcclh;
  NBLA_REGISTER_COMMUNICATOR_IMPL(DataParallelCommunicator, DataParallelCommunicatorNcclh, "cuda:half");
  typedef MultiProcessDataParallelCommunicatorNccl<Half> MultiProcessDataParallelCommunicatorNcclh;
  NBLA_REGISTER_COMMUNICATOR_IMPL(MultiProcessDataParallelCommunicator, MultiProcessDataParallelCommunicatorNcclh, "cuda:half");

#endif

  is_initialized = true;
}

void clear_cuda_memory_cache() {
  SingletonManager::get<Cuda>()->caching_allocator()->free_unused_caches();
}

/** Get CUDA array classes.
*/
vector<string> cuda_array_classes() {
  return SingletonManager::get<Cuda>()->array_classes();
}

/** Set CUDA array classes
*/
void _cuda_set_array_classes(const vector<string> &a) {
  return SingletonManager::get<Cuda>()->_set_array_classes(a);
}

void cuda_device_synchronize(const string &device) {
  cuda_set_device(std::stoi(device));
  NBLA_CUDA_CHECK(cudaDeviceSynchronize());
}

int cuda_get_device_count() {
  int count;
  NBLA_CUDA_CHECK(cudaGetDeviceCount(&count));
  return count;
}

vector<string> cuda_get_devices() {
  int count = cuda_get_device_count();
  vector<string> ret(count);
  for (int i = 0; i < count; ++i) {
    ret[i] = std::to_string(i);
  }
  return ret;
}

shared_ptr<void> cuda_create_stream(int device_id) {
  cuda_set_device(device_id);

  std::default_delete<cudaStream_t> default_deleter;
  auto deleter = [default_deleter](cudaStream_t* ptr) {
      NBLA_CUDA_CHECK(cudaStreamDestroy(*ptr));

      default_deleter(ptr);
  };

  auto stream = shared_ptr<cudaStream_t>(new cudaStream_t(), deleter);

  NBLA_CUDA_CHECK(cudaStreamCreateWithFlags(stream.get(), cudaStreamNonBlocking));

  return stream;
}

void* cuda_stream_shared_to_void(shared_ptr<void> stream) {
  auto s = static_cast<cudaStream_t*>(stream.get());

  return static_cast<void*>(*s);
}

void print_stream_flag (shared_ptr<void> stream) {
  auto s = static_cast<cudaStream_t*>(stream.get());
  unsigned int flags;

  NBLA_CUDA_CHECK(cudaStreamGetFlags(*s, &flags));
  printf("flags: %u\n", flags);
}

void print_stream_priority (shared_ptr<void> stream) {
  auto s = static_cast<cudaStream_t*>(stream.get());
  int p;

  NBLA_CUDA_CHECK(cudaStreamGetPriority(*s, &p));
  printf("priority: %d\n", p);
}

void cuda_nullstream_synchronize() {
  NBLA_CUDA_CHECK(cudaStreamSynchronize(0));
}

void cuda_stream_synchronize(shared_ptr<void> stream) {
  auto s = static_cast<cudaStream_t*>(stream.get());
  NBLA_CUDA_CHECK(cudaStreamSynchronize(*s));
}

void cuda_stream_destroy(shared_ptr<void> stream) {
  auto s = static_cast<cudaStream_t*>(stream.get());

  NBLA_CUDA_CHECK(cudaStreamDestroy(*s));
}

std::shared_ptr<void> cuda_create_event(int device_id) {
  cuda_set_device(device_id);

  std::default_delete<cudaEvent_t> default_deleter;
  auto deleter = [default_deleter](cudaEvent_t* ptr) {
      NBLA_CUDA_CHECK(cudaEventDestroy(*ptr));

      default_deleter(ptr);
  };

  auto event = shared_ptr<cudaEvent_t>(new cudaEvent_t(), deleter);

  NBLA_CUDA_CHECK(cudaEventCreateWithFlags(event.get(), cudaEventDisableTiming));

  return event;
}

void cuda_default_stream_event(shared_ptr<void> event){
  auto e = static_cast<cudaEvent_t*>(event.get());

  NBLA_CUDA_CHECK(cudaEventRecord(*e));

}
void cuda_stream_wait_event(shared_ptr<void> stream, shared_ptr<void> event) {
  auto s = static_cast<cudaStream_t*>(stream.get());
  auto e = static_cast<cudaEvent_t*>(event.get());

  NBLA_CUDA_CHECK(cudaStreamWaitEvent(*s, *e, 0));

}

void cuda_event_synchronize(shared_ptr<void> event) {
  auto e = static_cast<cudaEvent_t*>(event.get());

  NBLA_CUDA_CHECK(cudaEventSynchronize(*e));
}

}

