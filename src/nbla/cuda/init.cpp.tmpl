
// Copyright (c) 2017 Sony Corporation. All Rights Reserved.
// 
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// 
//     http://www.apache.org/licenses/LICENSE-2.0
// 
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// *WARNING*
// THIS FILE IS AUTO-GENERATED BY CODE GENERATOR.
// PLEASE DO NOT EDIT THIS FILE BY HAND!
// If you want to modify this file, edit following files.
// - src/nbla/cuda/init.cpp.tmpl
// - code_generator/generate.py

#include <nbla/init.hpp>
#include <nbla/cuda/init.hpp>
#include <nbla/cuda/cuda.hpp>
#include <nbla/cuda/common.hpp>
#include <nbla/array_registry.hpp>
#include <nbla/function_registry.hpp>
#include <nbla/array/cpu_array.hpp>
#include <nbla/cuda/array/cuda_array.hpp>
% for name, snake_name, _ in function_list:
% if name in function_types:
#include <nbla/cuda/function/${snake_name}.hpp>
% endif
% endfor
#include <nbla/solver_registry.hpp>
% for name, snake_name, _ in solver_list:
% if name in solver_types:
#include <nbla/cuda/solver/${snake_name}.hpp>
% endif
% endfor


#ifdef FEATURE_DIST_TRAIN
  #include <nbla/cuda/communicator/data_parallel_communicator.hpp>
  #include <nbla/cuda/communicator/multi_process_data_parallel_communicator.hpp>
#endif

#include <nbla/garbage_collector.hpp>

namespace nbla {

void init_cuda() {
  static bool is_initialized = false;
  if (is_initialized)
    return;

  // Init CPU features
  init_cpu();

  // Array registration
  // CudaArray
  NBLA_REGISTER_ARRAY_CREATOR(CudaArray);
  SingletonManager::get<Cuda>()->register_array_class("CudaArray");
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuArray, CudaArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CpuArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuCachedArray, CudaArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CpuCachedArray,
                                   synchronizer_cuda_array_cpu_array);

  // CudaCachedArray
  NBLA_REGISTER_ARRAY_CREATOR(CudaCachedArray);
  SingletonManager::get<Cuda>()->register_array_class("CudaCachedArray");
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuArray, CudaCachedArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CpuArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuCachedArray, CudaCachedArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CpuCachedArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CudaArray,
                                   synchronizer_default);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CudaCachedArray,
                                   synchronizer_default);

  // Function registration
% for name, _, arg_types in function_list:
  % for type_config, ttypes in function_types.get(name, {}).items():
    <%
    ttype_args = ', '.join(ttypes)
    ttype_symbol = ''.join(map(lambda x: x.replace(' ', ''), ttypes))
    function_sym = '{}{}<{}>'.format(name, "Cuda", ttype_args)
    function_typed_sym = '{}{}_{}'.format(name, "Cuda", ttype_symbol)
    %> 
  using ${function_typed_sym} = ${function_sym};
  NBLA_REGISTER_FUNCTION_IMPL(${name}, ${function_typed_sym}, "cuda:${type_config}"${''.join(map(lambda x: ', ' + x, arg_types))});
  % endfor
% endfor


  // Solver registration
% for name, _, arg_types in solver_list:
  % for type_config, ttypes in solver_types.get(name, {}).items():
    <%
    ttype_args = ', '.join(ttypes)
    ttype_symbol = ''.join(map(lambda x: x.replace(' ', ''), ttypes))
    solver_sym = '{}{}<{}>'.format(name, "Cuda", ttype_args)
    solver_typed_sym = '{}{}_{}'.format(name, "Cuda", ttype_symbol)
    %> 
  using ${solver_typed_sym} = ${solver_sym};
  NBLA_REGISTER_SOLVER_IMPL(${name}, ${solver_typed_sym}, "cuda:${type_config}"${''.join(map(lambda x: ', ' + x, arg_types))});
  %endfor  
% endfor

  // Communicator registration
#ifdef FEATURE_DIST_TRAIN
  typedef DataParallelCommunicatorNccl<float> DataParallelCommunicatorNcclf;
  NBLA_REGISTER_COMMUNICATOR_IMPL(DataParallelCommunicator, DataParallelCommunicatorNcclf, "cuda:float");
  typedef MultiProcessDataParallelCommunicatorNccl<float> MultiProcessDataParallelCommunicatorNcclf;
  NBLA_REGISTER_COMMUNICATOR_IMPL(MultiProcessDataParallelCommunicator, MultiProcessDataParallelCommunicatorNcclf, "cuda:float");
#endif

  is_initialized = true;
}

void clear_cuda_memory_cache() {
  SingletonManager::get<Cuda>()->memcache().clear();
}

/** Get CUDA array classes.
*/
vector<string> cuda_array_classes() {
  return SingletonManager::get<Cuda>()->array_classes();
}

/** Set CUDA array classes
*/
void _cuda_set_array_classes(const vector<string> &a) {
  return SingletonManager::get<Cuda>()->_set_array_classes(a);
}

void cuda_device_synchronize(const string &device) {
  cuda_set_device(std::stoi(device));
  NBLA_CUDA_CHECK(cudaDeviceSynchronize());
}

int cuda_get_device_count() {
  int count;
  NBLA_CUDA_CHECK(cudaGetDeviceCount(&count));
  return count;
}

vector<string> cuda_get_devices() {
  int count = cuda_get_device_count();
  vector<string> ret(count);
  for (int i = 0; i < count; ++i) {
    ret[i] = std::to_string(i);
  }
  return ret;
}
}

